---
title: "Results"
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

```{r set up, include=FALSE}
# Packages
library(here)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(ggplot2)
library(forcats)
library(patchwork)
library(gghighlight)

# Settings
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, 
                      message = FALSE, warning = FALSE,
                      eval.after = "fig.cap")
theme_set(theme_bw())

# Project
subproj_dir <- "hub-ensemble/"
here::i_am(paste0(subproj_dir, "analysis/results.Rmd"))
```

```{r model-description}
# Get latest evaluation scores for models
source(here(subproj_dir, "code", "get-model-eval.R"))

# Note - All model descriptions include the hub ensemble model
# Evaluation period
n_weeks <- max(model_eval$n)
start_date <- format.Date(eval_date - weeks(n_weeks), "%d %B")
end_date <- format.Date(eval_date, "%d %B %Y")

# Number of forecasters
n_model <- length(unique(model_eval$model))
n_team <- length(unique(model_eval$team_name))

# Number of models with rel wis scores
n_model_wis <- filter(model_eval, !is.na(rel_wis)) %>%
  distinct(model) %>% nrow()

## Not used
## Across all possible scores, which models never crossed >1 relative to baseline WIS?
# cross <- model_eval %>%
#   filter(!is.na(rel_wis)) %>%
#   group_by(model) %>%
#   summarise(n_scores = n(),
#             outperform = 1 - (sum(rel_wis >= 1, na.rm = TRUE)) / n_scores,
#             n_forecasts = sum(n))

```

We scored all forecasts submitted weekly in real time over the `r n_weeks` week period from `r start_date` to `r end_date`. Each week, forecasts could predict incident cases and deaths, for 32 locations over the following 4 weeks, creating 256 possible forecast targets. We received `r n_model` unique forecasting models from `r n_team` separate forecasting teams. We added an ensemble model using all available forecasts for each possible target. 

We used this dataset to create `r nrow(model_eval)` forecasting scores, each summarising a unique combination of model, variable, country, and week ahead horizon. Not all teams forecast for all targets, nor across all quantiles of the predictive distribution for each target. `r n_model_wis` models provided sufficient quantiles that we could evaluate them using the relative weighted interval score (WIS).

```{r}
#| figure-model-by-horizon,
#| fig.cap = "Figure #: Performance of short-term forecasts, by relative
#|  interval score (relative to a baseline forecast, top) and coverage of the 0.5
#|  interval (the proportion of observed values that fell within the predicted
#|  50% range, bottom). The scores of each model (grey) and ensemble (red) are
#|  averaged across all forecast targets and shown by one- to four-week ahead horizon."

# How often did the ensemble beat any score from all model scores?
n_ensemble <- filter(model_eval, !is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(),
            ensemble_beat = sum(ensemble_rel_wis < rel_wis, na.rm = TRUE),
            p_ensemble_beat = ensemble_beat / n * 100)
n_model_scores <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(n))
p_ensemble_beat <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(p_ensemble_beat) %>% round(0))

# PLOT: all models and ensemble performance by line over 1:4 week horizon
# average scores across locations for each model
h1_h4 <- model_eval %>%
  group_by(is_hub, model, horizon, target_variable) %>%
  summarise(is_hub = any(is_hub),
            n = n(),
            mean_wis = mean(rel_wis, na.rm = TRUE),
            sd_wis = sd(rel_wis, na.rm = TRUE),
            mean_cov50 = mean(cov_50, na.rm = TRUE),
            mean_cov95 = mean(cov_95, na.rm = TRUE))

# rel WIS per model by horizon
plot_model_wis <- h1_h4 %>%
  ggplot(aes(x = horizon, y = mean_wis, colour = model)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 1), lty = 2, col = "black") +
  # highlight ensemble in red and add label to plot
  gghighlight(is_hub, calculate_per_facet = TRUE, use_direct_label = F) +
  labs(y = "Mean relative WIS", x = NULL) +
  scale_color_brewer(type = "qual", palette = 6) +
  facet_wrap(~ target_variable, scales = "fixed") +
  theme(strip.background = element_blank(),
        legend.position = "none",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) 

# Plot coverage by horizon
plot_model_coverage <- h1_h4 %>%  
  # show only 0.5 coverage level in plot
  mutate(expected = 0.5) %>%
  ggplot(aes(x = horizon, y = mean_cov50, colour = model)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = expected), lty = 2, colour = "black") +
  gghighlight(is_hub, calculate_per_facet = TRUE, use_direct_label = F) +
  scale_color_brewer(type = "qual", palette = 6) +
  labs(y = "50% coverage", x = "Weeks ahead horizon",
       fill = NULL, colour = NULL) +
  facet_wrap(~ target_variable, scales = "fixed") +
  theme(strip.background = element_blank(),
        strip.text = element_blank(),
        legend.position = "bottom") 

# Plot together
plot_model_wis +
  plot_model_coverage +
  plot_layout(nrow = 2)

```

The ensemble model performed well compared to both the relative scores of each original forecast model scaled to the baseline, and the baseline model itself. In ranking all models' scores compared to the baseline, the ensemble performed better on relative WIS than `r p_ensemble_beat$Cases`% model scores when forecasting cases (n=`r n_model_scores$Cases`), and `r p_ensemble_beat$Deaths`% of scores for forecasts of incident deaths (n=`r n_model_scores$Deaths`). Compared to the baseline model alone, the ensemble outperformed at one week for both cases and deaths (figure #). For horizons longer than one-week ahead, performance depended on the epidemiological target. Using relative WIS, the ensemble stopped outperforming the baseline at three to four weeks for cases. In contrast, the ensemble outperformed the baseline for deaths at all horizons considered (up to four weeks WIS), with no discernible deterioration in performance. 

We observed similar trends in performance when considering how well the ensemble calibrated to uncertainty. At one week the case ensemble was well calibrated (ca. 50% nominal coverage), but this did not hold with increasing forecast horizon. The death ensemble was well calibrated at the 95% level for all horizons and under confident for the 50% level, with only slow deterioration with increasing forecast horizon.

```{r}
#| figure-model-by-location,
#| fig.cap = "Figure #: Performance of short-term forecasts across models and
#|  median ensemble (asterisk), by country, forecasting cases (top) and deaths
#|  (bottom) for two-week ahead forecasts. Performance measured by relative
#|  weighted interval score scaled against baseline. Boxplots show interquartile
#|  range, with outliers as faded points, and ensemble model performance marked
#|  by asterisk."

# How often did the ensemble beat the baseline across locations?
n_ensemble_loc <- model_eval %>%
  filter(is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(), # n = 32 countries for each target
            p_ensemble_beat_loc = sum(rel_wis <= 1, na.rm = TRUE) / n * 100)
p_ensemble_beat_loc <- map(split(n_ensemble_loc, n_ensemble_loc$target_variable),
                       ~ .x %>% pull(p_ensemble_beat_loc) %>% round(0))

# PLOT: All model and ensemble performance by boxplot by location at 2 wk horizon
model_eval %>%
  # Use 1 week horizon
  filter(horizon == 2) %>%
  # plot structure: boxplot rel wis by location and horizon
  ggplot(aes(x = location_name, y = rel_wis,
             colour = target_variable,
             fill = target_variable)) +
  geom_boxplot(alpha = 0.8, 
               outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 1), lty = 2) +
  # overlay ensemble as extra point
  geom_point(aes(y = ensemble_rel_wis),
              size = 2, shape = "asterisk",
             colour = "grey 10",
             position = position_dodge(width = 0.8)) +
  # format
  ylim(c(0,4)) +
  labs(x = NULL, y = "Scaled relative WIS across models") +
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1") +
  facet_wrap(~ target_variable, scales = "fixed", nrow = 2) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 30, hjust = 1),
        strip.background = element_blank())

# Ensemble relative to IQR for each country
# ensemble_iqr <- model_eval %>%
#   group_by(location, target_variable) %>%
#   summarise(q.25 = quantile(rel_wis, 0.25, na.rm = TRUE),
#             median = median(rel_wis, na.rm = TRUE),
#             q.75 = quantile(rel_wis, 0.75, na.rm = TRUE),
#             ensemble = median(ensemble_rel_wis) < q.25)

```

The ensemble also performed consistently well when forecasting across countries, relative to individual models and the baseline (figure #).  Compared to models that forecast across all 32 countries at the two week horizon, the ensemble was the most consistent in outperforming the baseline across countries compared to any single model forecasting deaths, and all but one model for case forecasts. Considering forecast targets across all 32 countries and over all four horizons (128 targets), the ensemble forecast outperformed the baseline for `r p_ensemble_beat_loc$Cases`% and `r p_ensemble_beat_loc$Deaths`% of all 128 targets when forecasting cases and deaths respectively.

```{r}
#| figure-alternative-ensembles,
#| fig.cap = paste0(
#| "Figure #: Performance of alternative ensemble methods at 2 week
#|  horizon, showing mean difference (triangle) in relative weighted
#|  interval score, with 48% and 96% probability (thick and thin line
#|  respectively). The difference in WIS is a comparison of
#|  scores from forecasts made from all possible combinations of methods,
#|  with a single element of ensemble method input changed.
#|  Reference categories are: 
#|  weighted v. unweighted (n=", 
#|  ensemble_change_n$Weighted, 
#|  "); median v. mean (n=", 
#|  ensemble_change_n$Median, 
#|  "); cutoff by WIS v. all models included (n=", 
#|  ensemble_change_n$`Cutoff rel. WIS < 1`, 
#|  "); relative WIS measures over 10 weeks of forecast history vs. all forecasts (n=",
#|  ensemble_change_n$`10 weeks history`, ")")
#
# OR-style plot showing relative impact of different ensemble methods on performance
# get eval, calculate differences between ensemble methods
source(here::here(subproj_dir, "code", "get-ensemble-eval.R"))
# plot differences summary
ggplot(ensemble_change_dtb, aes(x = change)) +
  # mean = triangle
  geom_point(aes(y = mean), pch = 2, size = 4) +
  # 48% interval = thick line
  geom_linerange(aes(ymin = low_48, ymax = high_48), lwd = 2) +
  # 96% interval = thin line
  geom_linerange(aes(ymin = low_96, ymax = high_96), lwd = 1) +
  # 0 change in wis = reference line
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = NULL, y = "Change in relative WIS compared to reference") +
  coord_flip()

```

At the two-week ahead horizon, variations in ensemble methods made little difference to forecast scores. Ensembles that weighted forecasts showed no difference in performance to simple unweighted ensemble methods. Similarly, in choosing a method with which to weight forecasts, the choice of whether the use scores across all past forecasts, or scores evaluating only the most recent 10 weeks' forecast scores, made very little difference to the performance of the resulting ensemble (0 mean change in forecast score). The choice to exclude any forecast that scored worse than the baseline forecast ("cut off") affected the performance of the ensemble in both directions, overall slightly worsening performance (+`r ensemble_change_dtb[ensemble_change_dtb$change == "Cutoff rel. WIS < 1", "mean"]` relative WIS).  Using the median average was the only variation of ensemble method that typically improved performance, compared to using the mean average across any combination of ensemble method.

